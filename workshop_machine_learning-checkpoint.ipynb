{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - identificação de dígitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As técnicas de machine learning divergem das técnicas de programação normal, porque fazem com que os programas em si \"aprendam\" (reconheçam padrões) sem serem explicitamente programados (sem ser o programador a escrever todos os outputs para cada entrada).\n",
    "\n",
    "Para que esta aprendizagem efetivamente se desenvolva, é necessária uma grande quantidade de dados e capacidade de processamento para os algoritmos de machine learning. Note-se que estes algoritmos têm uma forte vertente estatística, o que permite, por um lado, saber como fazer o modelo evoluir e, por outro, atestar a sua evolução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste workshop iremos falar e usar 3 modelos de machine learning para a identificação de dígitos escritos. :\n",
    "\n",
    "1-RandomTreeflorestClassifier \n",
    "\n",
    "2-LogisticRegression\n",
    "\n",
    "3-CNN\n",
    "\n",
    "Vamos usar a base de dados disponível em https://www.kaggle.com/c/digit-recognizer/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos dados e vizualização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ler os dados e manipulá-los, vamos usar a biblioteca pandas (biblioteca que cria e manipula estrutras de dados com uma grande performance).Para vizualizar dados iremos usar o matplotlib e o skimage.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df=shuffle(df)\n",
    "\n",
    "y = df.loc[:,'label'].values\n",
    "x = df.drop(\"label\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Numero de colunas,pizeis por imagem e label){}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(5,15, figsize=(15,10))\n",
    "for j in range(15):\n",
    "    for i in range(5):\n",
    "        ax1[i][j].imshow(x[i+15*j].reshape((28,28)),'gray')\n",
    "        ax1[i][j].axis('off')\n",
    "        ax1[i][j].set_title(y[i+15*j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar um modo de diminuir a dimensão dos dados, substituindo o gradiente de cor por uma variável binaria (0 se luminosidade inferior a 125 e 256 se superior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_red = y\n",
    "x_red = x\n",
    "\n",
    "#for line in range (x_train_red.shape[0]):\n",
    "#    for col in range (x_train_red.shape[1]):\n",
    "#        if(x_train_red[line][col] <128):\n",
    "#            x_train_red[line][col]=0\n",
    "#        else:\n",
    "#            x_train_red[line][col]=256\n",
    "\n",
    "x_red=np.vectorize(lambda a:0 if a<128 else 256)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(5,15, figsize=(15,10))\n",
    "for j in range(15):\n",
    "    for i in range(5):\n",
    "        ax1[i][j].imshow(x_red[i+15*j].reshape((28,28)),'gray')\n",
    "        ax1[i][j].axis('off')\n",
    "        ax1[i][j].set_title(y[i+15*j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um algoritmo de machine learning de classificação (neste caso especifico queremos classificar as imagens com um dos 10 dígitos) é um algoritmo que recebe um train_dataset com as respetivas tags (dígitos que correspondem a imagens) e cria o modelo de decisão com o objetivo de prever corretamente o resultado do test_dataset. Para corretamente avaliar a sua performance será necessário dividi-lo em train e test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_train=x[0:35000,:]\n",
    "y_train=y[0:35000]\n",
    "\n",
    "x_train_red=x_red[0:35000,:]\n",
    "y_train_red=y_red[0:35000]\n",
    "\n",
    "x_test=x[35000:,:]\n",
    "y_test=y[35000:]\n",
    "\n",
    "x_test_red=x_red[35000:,:]\n",
    "y_test_red=y_red[35000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomTreeFlorestClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tal como o nome indica, uma decision tree está estruturada em forma de árvore e organiza a sua decisão num esquema composto pelo topo da árvore (dataset), por nós(condições que subdividem o dataset) e por folhas (o final de cada ramo onde se encontra a previsão para cada dado que sair).Tomemos como exemplo o seguinte dataset simplificado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo da estrutura:\n",
    " \n",
    "<img src=\"tree_example.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=[\"sexo\",\"num Sbs\",\"idade>9.5 ?\",\"survived\"]\n",
    "dat=[\n",
    "     [\"M\",3,1 ,0 ],\n",
    "     [\"M\",2,1,0 ],\n",
    "     [\"M\",1,1 ,0],\n",
    "     [\"M\",1,0,0],\n",
    "     [\"M\",2,0,1],\n",
    "     [\"F\",2,0,1],\n",
    "     [\"F\",1,1,1 ],\n",
    "     [\"F\",1,1,1 ],\n",
    "     [\"F\",0,1,1 ],\n",
    "     [\"F\",0,1,0 ]\n",
    "    \n",
    "]\n",
    "example_df=pd.DataFrame(data=dat,columns=col)\n",
    "print(\"Dados para desenhar uma ClassificationTree:\")\n",
    "example_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo que vamos ver utiliza a técnica greedy de partição do dataset, que utiliza como critério o corte que diminui mais a entropia do dataset. Para saber mais sobre entropia: http://www.saedsayad.com/decision_tree.htm .Para além disso para aumentar o grau de confiança no resultado este algoritmo não cria uma, mas várias decisiontrees para garantir uma decisão mais robusta.(menor desvio padrão)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x,y):\n",
    "    tot = x+y\n",
    "    p1 = x/tot\n",
    "    p2 = y/tot\n",
    "    return -p1*np.log2(p1)-p2*np.log2(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculo da entropia :\n",
    "\\begin{equation}\\label{Cálculo da entropia}\n",
    "    E(S)=\\sum_{i=1}^{c}-p_{i}\\log_{2}{p_{i}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcao da entropia:\n",
    " \n",
    "<img src=\"entropia.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entropia com 1 var:\")\n",
    "col=[\"Survived\",\"died\"]\n",
    "dat=[\n",
    "    [5,5]]\n",
    "    \n",
    "\n",
    "example_df=pd.DataFrame(data=dat,columns=col)\n",
    "print(\"Dados para desenhar uma ClassificationTree:\")\n",
    "display(example_df)\n",
    "\n",
    "print(\"entropia: {}\".format(entropy(5,5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculo da entropia 2 atributos:\n",
    "\\begin{equation}\\label{Cálculo da entropia 2 atributos}\n",
    "    E(T,X)=\\sum_{c}P(c)E(c)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entropia de 2 var:\")\n",
    "col=[\"Survived\",\"died\"]\n",
    "idx =[\"Male\",\"Female\"]\n",
    "dat=[\n",
    "    [1,4],\n",
    "    [4,1]\n",
    "]\n",
    "    \n",
    "    \n",
    "\n",
    "example_df=pd.DataFrame(index=idx,data=dat,columns=col)\n",
    "print(\"Dados para desenhar uma ClassificationTree:\")\n",
    "display(example_df)\n",
    "\n",
    "print(\"Entropia E(male)= E(1,4) = {}\".format(entropy(1,4)))\n",
    "print(\"Entropia E(female)= E(4,1) = {}\".format(entropy(4,1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ganho de informacao\n",
    "\\begin{equation}\\label{Cálculo da entropia gznho de informacao}\n",
    "    G(T,X) = E(T) - E(T,x)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crianção do modelo RandomTree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.metrics import accuracy_score as ac_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = RFC(max_depth=17,criterion='entropy')\n",
    "tree_classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver train score\n",
    "tree_predictions=tree_classifier.predict(x_train)\n",
    "ac_score(y_train,tree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver test score\n",
    "tree_predictions=tree_classifier.predict(x_test)\n",
    "ac_score(y_test,tree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(5,15, figsize=(15,10))\n",
    "for j in range(15):\n",
    "    for i in range(5):\n",
    "        ax1[i][j].imshow(x_test[i+15*j].reshape((28,28)))\n",
    "        ax1[i][j].axis('off')\n",
    "        ax1[i][j].set_title(tree_predictions[i+15*j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errados_img = x_test[tree_predictions != y_test]\n",
    "errados_lbl = y_test[tree_predictions != y_test]\n",
    "errados_pred_lbl = tree_predictions[tree_predictions != y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicoes falhadas\n",
    "fig1, ax1 = plt.subplots(5,15, figsize=(15,10))\n",
    "\n",
    "for j in range(15):\n",
    "    for i in range(5):\n",
    "        ax1[i][j].imshow(errados_img[i+15*j].reshape((28,28)))\n",
    "        ax1[i][j].axis('off')\n",
    "        ax1[i][j].set_title(\"(T {})(F {})\".format(errados_lbl[i+15*j],errados_pred_lbl[i+15*j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msm coisa para o reduzido\n",
    "tree_classifier = RFC(max_depth=2000)\n",
    "tree_classifier.fit(x_train_red,y_train_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atestar train score\n",
    "tree_predictions=tree_classifier.predict(x_train_red)\n",
    "ac_score(y_train_red,tree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atestar test score\n",
    "tree_predictions=tree_classifier.predict(x_test_red)\n",
    "ac_score(y_test_red,tree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(5,15, figsize=(15,10))\n",
    "for j in range(15):\n",
    "    for i in range(5):\n",
    "        ax1[i][j].imshow(x_test_red[i+15*j].reshape((28,28)))\n",
    "        ax1[i][j].axis('off')\n",
    "        ax1[i][j].set_title(tree_predictions[i+15*j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic_classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo é o modelo mais usado em áreas de investigação aplicadas (investigações médicas, farmacêuticas, ciências sociais).A vantagem que tem em relação aos outros é ser um modelo amplamente aceite e com critérios de creditação do modelo já muito estudados e ajustáveis para uma grande diversidades de datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função sigmoid:\n",
    "<img src=\"logistic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que o modelo faz, a duas dimensões, é encontrar a curva com a forma de cima que melhor se ajusta aos dados encontrados e depois, para cada observação, calcula o resultado da função nesse ponto, que é diretamente traduzido numa probabilidade (a função tem sempre valores entre 0 e 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo :\n",
    "<img src=\"log_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando o problema tem mais do que uma variável resultado (neste caso há 10). O modelo usa inferência estatística, recorrendo ao teorema de Bayes como ponto inicial para a classificação (calcula qual a probabilidade de ser 0 face à de ser 1 e faz isso para todas as possibilidades, e procura o modelo que maximize a probabilidade total de sucesso).Para mais informações, um bom ponto de partida é https://en.wikipedia.org/wiki/Multinomial_logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do modelo logit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset mt extenso para este método diminuir dados\n",
    "# +- 1 min a correr\n",
    "x_train_red_1=x_train_red[0:5000,:]\n",
    "y_train_red_1=y_train_red[0:5000]\n",
    "\n",
    "#idiota erro gravissimo\n",
    "x_test_red_1=x_test_red[0:500]\n",
    "y_test_1=y_test[0:500]\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logi_classifier=LogisticRegression(verbose =50)\n",
    "logi_classifier.fit(x_train_red_1,y_train_red_1)\n",
    "prediction=logi_classifier.predict(x_test_red_1)\n",
    "ac_score(y_test_1,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(5,15, figsize=(15,10))\n",
    "for j in range(15):\n",
    "    for i in range(5):\n",
    "        ax1[i][j].imshow(x_test_red_1[i+15*j].reshape((28,28)))\n",
    "        ax1[i][j].axis('off')\n",
    "        ax1[i][j].set_title(prediction[i+15*j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as c_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_matrix(y_test_1,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meter aqui quantos 0 foram classficados como 1...2..9 etc\n",
    "\n",
    "matrix=np.zeros((10,10))\n",
    "\n",
    "for line in range(10):\n",
    "    unique, counts=np.unique(prediction[y_test_1 == line],return_counts=True)\n",
    "    matrix[line][unique]=counts\n",
    "    \n",
    "viz=pd.DataFrame(index=[0,1,2,3,4,5,6,7,8,9],columns=[0,1,2,3,4,5,6,7,8,9],data=matrix)\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes de Convolução Neuronais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As redes de convulsão neuronais, apesar de serem já conhecidas desde 1950, só começaram a ter a atenção do mundo de machine learning quando, em 2012, Ciresan et al melhoraram muito o melhor modelo na literatura para identificação de imagens. Já muitas tentativas foram feitas para utilizar este modelo noutro tipos de modelos (que não reconhecimento de imagem ou vídeo) sem muito sucesso. Este modelo é muito pesado computacionalmente.\n",
    " \n",
    "Decidi abordar este modelo por ser intuitivo no modo como funciona e ter uma alta performance. Para ter um entendimento mais aprofundado sobre o tema, podem ler este paper da autoria de Jianxin Wu: https://pdfs.semanticscholar.org/450c/a19932fcef1ca6d0442cbf52fec38fb9d1e5.pdf. Para saber mais sobre polling, podem ler este paper da autoria de Dominic Shrerer http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNN_image.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operador Convolução (usado para detectar features nos dados):\n",
    "\\begin{equation}\\label{Convulusãp}\n",
    "   f(t)*g(t)=\\int_{y=-\\infty}^{+\\infty} f(y)*g(t-y)dy\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<img src=\"convulucao/Diapositivo1.JPG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para observar os efeitos de diferentes feature detectores, iremos ver a documentação do programa Gimp (um programa de edição de imagem):\n",
    "https://docs.gimp.org/en/plug-in-convmatrix.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling (usado para diminuir a complexidade dos dados e dar mais relacionamento estrutural):\n",
    "\n",
    "\n",
    "<img src=\"convulucao/Diapositivo2.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver tudo em ação: http://scs.ryerson.ca/~aharley/vis/conv/flat.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A party fully connected não será abordada neste workshop. Para saberem mais, podem ver o research paper referido supramencionado ou numa próxima edição deste workshop mediante sugestões. (também estou aberto a questões sobre esta parte no fim do workshop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contruindo aCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como nos outros casos, começaremos com as importações das bibliotecas a usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construir a rede em si:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Convulução:\n",
    "#foram escolhidos 32 features detectores\n",
    "classifier = Sequential()\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape = (28, 28,1), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding a second convolutional layer\n",
    "classifier.add(Convolution2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Convolution2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 28*2, activation = 'relu'))\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 28*2, activation = 'relu'))\n",
    "classifier.add(Dense(units = 10, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer fit dos dados à rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0,\n",
    "#validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, \n",
    "#steps_per_epoch=None, validation_steps=None)\n",
    "\n",
    "X_train=x_train.reshape(-1,28,28,1)\n",
    "x_test_conv=x_test.reshape(-1,28,28,1)\n",
    "Y_train = to_categorical(y_train, num_classes = 10)\n",
    "\n",
    "classifier.fit(X_train, Y_train, batch_size = 90, epochs = 5,validation_split=0.1, verbose = 2 ,shuffle=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the validation dataset\n",
    "Y_pred_CNN = classifier.predict(x_test_conv)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred_CNN,axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_score(y_test,Y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz=pd.DataFrame(index=[0,1,2,3,4,5,6,7,8,9],columns=[0,1,2,3,4,5,6,7,8,9],data=c_matrix(y_test,Y_pred_classes))\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver os erros cometidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - y_test != 0)\n",
    "\n",
    "fig1, ax1 = plt.subplots(10,5, figsize=(15,40))\n",
    "for j in range(5):\n",
    "    for i in range(10):\n",
    "        ax1[i][j].imshow(x_test[errors][i+10*j].reshape((28,28)),'gray')\n",
    "        ax1[i][j].axis('off')\n",
    "        title=\"pr: \"+(str)(Y_pred_classes[errors][i+10*j]) +\" Re: \"+(str)(y_test[errors][i+10*j])\n",
    "        ax1[i][j].set_title(title)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neuronais bases matemáticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rede neuronal feed forward esquema:\n",
    "\n",
    "\n",
    "<img src=\"NeuralNetwork_img.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relacao entre as variaveis:\n",
    "\\begin{equation}\\label{feed_forward}\n",
    "   z_j^{(l)} = \\sum_{i}y_i^{(l-1)}w_{ij}^{(l-1)} \n",
    "\\end{equation}\n",
    "\n",
    "introducao da nao linearidade:\n",
    "\\begin{equation}\\label{feed_f_y}\n",
    "   y_j^{(l)} = N{(z_j^{(l)})} \n",
    "\\end{equation}\n",
    "\n",
    "Previsão:\n",
    "\\begin{equation}\\label{prev}\n",
    "  Lprev_i = y_i^L\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas como aprende? Como qualquer outro algoritmo de machine learning tem como objetivo minimizar uma função objetivo que representa o quão perto as suas previsões estão da realidade. Tomemos por exemplo como função a minimizar R2. Sendo  $Ltrue_i$ o valor real do dado i e sendo $Lprev_i$ o valor previstopela nossa rede prevê:\n",
    "(escolheu-se na explicação teórica não nos focarmos no exemplo de classificação , uma vez que traria algumas complicações na matemática , é mais fácil escrever a derivaçãõ uma quadrática da função soft max na activação com função objetivo entropia)\n",
    "\n",
    "função a minimizar para apenas um exemplo:\n",
    "\\begin{equation}\\label{feed_f_y}\n",
    "  error = \\sum_{i}{(Lprev_i-Ltrue_i)^2}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar técnica de gradient descend para minimizar este erro:  (Passos: calcular o gradiente:) Começar por calcular o gradiente da ultima camada\n",
    "\n",
    "\\begin{equation}\\label{feed_f_y}\n",
    "  \\frac{\\partial error}{\\partial y_i^L} = {2(y_i^L-Ltrue_i)}\n",
    "\\end{equation}\n",
    "\n",
    "Como se propaga para o $ Z^L $ ?\n",
    "\\begin{equation}\\label{prop_zL}\n",
    " \\delta_i^{(L)} = \\frac{\\partial error}{\\partial z_i^L} =  \\frac{\\partial error}{\\partial y_i^L}\\frac{\\partial y_i^L}{\\partial z_i^L} = {2(y_i^L-Ltrue_i)}N'(z_i^L)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation}\\label{fim_antes}\n",
    " \\delta_i^{(l-1)} =  \\frac{\\partial error}{\\partial z_{i}^{l-1}} = \\sum_j \\frac{\\partial error}{\\partial z_j^{l}}\n",
    " \\frac{\\partial z_j^{l}}{\\partial z_i^{l-1}} \n",
    " =  \\sum_j \\delta_j^{(l)} \\frac{\\partial z_j^{l}}{\\partial y_i^{l-1}}\\frac{\\partial y_i^{l-1}}{\\partial z_i^{l-1}}\n",
    " =  \\sum_j \\delta_j^{(l)} w_{ij}^{(l-1)} N'(z_i^{(l-1)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim o que interessa é atualizar os pesos ? como se sabe a derivada do peso $w_{ij}^{(l)}$ ?\n",
    "\n",
    "\\begin{equation}\\label{fim}\n",
    " \\frac{\\partial error}{\\partial w_{ij}^l} = \\frac{\\partial error}{\\partial z_j^{l+1}}\n",
    " \\frac{\\partial z_j^{l+1}}{\\partial w_{ij}^l} =  \\delta_j^{(l+1)}y_i^{l}\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
